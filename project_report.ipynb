{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c38ca2-b308-47f0-be51-c928e1827831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import altair as alt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Disables maximum rows allowed for altair plots\n",
    "# alt.data_transformers.disable_max_rows()\n",
    "# Uncomment below to re-enable max rows\n",
    "# alt.data_transformers.enable('default', max_rows=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eed57d-35bc-445c-8349-8d59132b82cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://drive.google.com/file/d/1dTmTAiRGM5skZzMb9NwpOkcQrY0Dpq6t/view?usp=sharing\"\n",
    "url = 'https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "diabetes = pd.read_csv(url) #read data\n",
    "display(diabetes)\n",
    "display(diabetes.info())\n",
    "diabetes[\"diabetes\"].value_counts(normalize = True) #show classification variable distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d71b52-0d33-4754-bc56-9f5d83f12c85",
   "metadata": {},
   "source": [
    "Next we need to resample the data to create an even distribution of positive and negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d299c96-f3cb-4de4-a24c-d91a054c01e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(1) # set seed\n",
    "\n",
    "diabetes_negative = diabetes[diabetes[\"diabetes\"] == 0] #create even amounts of positive and negative labels\n",
    "diabetes_positive = diabetes[diabetes[\"diabetes\"] == 1]\n",
    "diabetes_negative_downscaled = resample(\n",
    "    diabetes_negative, n_samples = diabetes_positive.shape[0]\n",
    ")\n",
    "diabetes_negative_downscaled.shape[0]\n",
    "diabetes_downsampled = pd.concat((diabetes_positive, diabetes_negative_downscaled))\n",
    "display(diabetes_downsampled[\"diabetes\"].value_counts(normalize = True))\n",
    "diabetes_downsampled.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876e93c7-0e3c-4cfd-9c4c-ff14bb8b5344",
   "metadata": {},
   "source": [
    "Now that the data is resampled we can create the train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5160a2-1cc4-4891-a873-d7935fa6476e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diabetes_train, diabetes_test = train_test_split(\n",
    "    diabetes_downsampled, train_size = .75, stratify = (diabetes_downsampled[\"diabetes\"]), random_state=42 # split data\n",
    ")\n",
    "display(diabetes_train.info())\n",
    "diabetes_train[\"diabetes\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6752515-f9ec-4926-a809-515c1e656615",
   "metadata": {},
   "source": [
    "Now we filter the data to the numeric columns to aggregate and observe trends, and then do the same with categorical values using ``value_counts``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979e358-af1d-4858-bc7a-4c6cd8f4790f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diabetes_stats_downsample = diabetes_downsampled.drop([\"gender\", \"hypertension\", \"smoking_history\", \"diabetes\"], axis=1) # find mean values\n",
    "display(diabetes_stats_downsample.agg([\"mean\",\"std\"]))\n",
    "diabetes_stats = diabetes_train.drop([\"gender\", \"hypertension\", \"smoking_history\", \"diabetes\"], axis=1) # find mean values\n",
    "display(diabetes_stats.agg([\"mean\",\"std\"])) #show average + variability demographics for survey\n",
    "#display(diabetes[\"gender\"].value_counts(normalize = True))\n",
    "#display(diabetes[\"hypertension\"].value_counts(normalize = True))\n",
    "#display(diabetes[\"smoking_history\"].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245149c0-c962-4f3e-bd74-ab09e0594ed1",
   "metadata": {},
   "source": [
    "Next we'll make the preprocessor to use in K Means Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a7fb57-e8ee-4f68-aca3-38bd89d4bc33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_names = [\"age\", \"bmi\", \"HbA1c_level\", \"blood_glucose_level\"]\n",
    "\n",
    "diabetes_preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), feature_names),\n",
    ")\n",
    "diabetes_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7e034-48df-4371-8c08-79cc90809cf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diabetes_preprocessor.fit(diabetes)\n",
    "diabetes_scaled = diabetes_preprocessor.transform(diabetes)\n",
    "diabetes_scaled_df = pd.DataFrame(diabetes_scaled, columns=feature_names)\n",
    "diabetes_scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10081d2-6406-4405-87b7-4d81c18deb06",
   "metadata": {},
   "source": [
    "Using our preprocessor, we can train a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0009b2c-9295-4d4b-a2bb-853ba1ec4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_pipe_knn = make_pipeline(diabetes_preprocessor, KNeighborsClassifier())\n",
    "\n",
    "X_train = diabetes_train[[\"age\", \"bmi\", \"HbA1c_level\", \"blood_glucose_level\"]]\n",
    "y_train = diabetes_train[\"diabetes\"]\n",
    "\n",
    "cv_scores = cross_val_score(diabetes_pipe_knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "cv_scores_std = np.std(cv_scores)\n",
    "print(\"average cv score:\", cv_scores.mean(), \"±\", cv_scores_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f300cff4-76c7-44e8-8fb4-670093059458",
   "metadata": {},
   "source": [
    "Here, we see a score of __ . This is fine, however let's see if we can improve our score by tuning the `n_neighbors` hyperparameter in `KNeighborsClassifier()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30478f46-ebdc-4cfa-a48f-0ca88f177389",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization\n",
    "Now, we'll make a GridSearch CV object and a range of potential K values to find the best K value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46635c-f364-4c52-b20d-b7637b0ac746",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diabetes_grid = {\n",
    "    \"kneighborsclassifier__n_neighbors\"  : range(\n",
    "        1,60, 2),\n",
    "}\n",
    "diabetes_pipe = make_pipeline(diabetes_preprocessor, KNeighborsClassifier())\n",
    "diabetes_grid = GridSearchCV(\n",
    "    estimator = diabetes_pipe,\n",
    "    param_grid = diabetes_grid,\n",
    "    cv = 5\n",
    ")\n",
    "accuracies_grid = pd.DataFrame(\n",
    "    diabetes_grid.fit(\n",
    "        diabetes_train[[\"age\", \"bmi\", \"HbA1c_level\", \"blood_glucose_level\"]],\n",
    "        diabetes_train[\"diabetes\"],\n",
    "    ).cv_results_\n",
    ")\n",
    "accuracies_grid = (\n",
    "    accuracies_grid[[\n",
    "        \"param_kneighborsclassifier__n_neighbors\",\n",
    "        \"mean_test_score\",\n",
    "        \"std_test_score\"\n",
    "    ]]\n",
    "    .assign(sem_test_score=accuracies_grid[\"std_test_score\"] / 10**(1/2))\n",
    "    .rename(columns={\"param_kneighborsclassifier__n_neighbors\": \"n_neighbors\"})\n",
    "    .drop(columns=[\"std_test_score\"])\n",
    ")\n",
    "accuracies_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4746781c-855a-43b5-b3f7-0d333bc350e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy = alt.Chart(accuracies_grid).mark_line(point=True).encode(\n",
    "    x = alt.X(\"n_neighbors\"),\n",
    "    y = alt.Y(\"mean_test_score\", title='Mean Test Score', scale=alt.Scale(zero=False)),\n",
    ").properties(\n",
    "    title='Ideal n_neighbors value based off Mean Test Score',\n",
    "    width=600\n",
    ")\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0361c9-2c0d-4f2f-8803-135596206294",
   "metadata": {
    "tags": []
   },
   "source": [
    "Based off `accuracies_grid` and our accuracy plot, we can see that `KNeighborsClassifier()` will perform best when `n_neighbors = 31` based off mean test score. Although `n_neighbors = 33` has a high `sem_test_score` such that there may be more variability in the mean test score, it will likely perform better than all other `n_neighbors` values up to `n_neighbors = 60`. There is a possibility of a better `n_neighbors` value past 60, however we will be unable to determine such value due to limited computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8f6920-9832-43d0-a9a8-9c97b0b56145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diabetes_pipe_knn_31 = make_pipeline(diabetes_preprocessor, KNeighborsClassifier(n_neighbors=31))\n",
    "\n",
    "X_train = diabetes_train[[\"age\", \"bmi\", \"HbA1c_level\", \"blood_glucose_level\"]]\n",
    "y_train = diabetes_train[\"diabetes\"]\n",
    "\n",
    "cv_scores = cross_val_score(diabetes_pipe_knn_31, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "cv_scores_std = np.std(cv_scores)\n",
    "print(\"average cv score:\", cv_scores.mean(), \"±\", cv_scores_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a44329-dd12-415e-9be8-964fc1d73c47",
   "metadata": {
    "tags": []
   },
   "source": [
    "After performing a cross-validation where cv=5, our model has an accuracy of 0.89655 with a standard deviation of 0.00167. A high accuracy may indicate that our model is able to correctly make predictions, and a low standard deviation indicates that our model is likely not underfitting nor overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39ae24-c222-463b-a53f-e904cdfe1e99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f66cb07-b96c-455f-ac6a-5a42230a0f62",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "So far, we have found a feature set that allows us to train a relatively decent model. However, we would like to see whether there is another pool of features which may produce a more accurate model. To do this, we will separate our data into numerical features, binary features, and categorical features. We will also drop 'gender', as it may introduce bias into our final model. We could choose to represent 'gender' as a binary feature or an ordinal feature, but we believe it is best to not to include it at all.\n",
    "\n",
    "We will pass all numerical features through `StandardScaler()` like previously, but this time we will pass categorical features (`smoking_history`) through `OrdinalEncoder()`, as we will choose to interpret smoking history as different levels of smoking intensity (never < former < current). However, we would also like to note that that 'no info' may affect our results, and using `OneHotEncoder()` is another option to consider in the future.\n",
    "\n",
    "As for our binary features, we will use 'passthrough' as they do not need any additional processing to be used in `KNeighborsClassifier()`.\n",
    "\n",
    "Lastly, we need to consider that the previous `n_neighbors=31` hyperparamater that we found is specific to the feature set containing only numerical features. Thus, we will use the default value of `n_neighbors=5` in our `KNeighborsClassifier()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec42ce-7349-446d-b4e4-eb6f44fa8a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = diabetes_train.drop(columns=['diabetes', 'gender'])\n",
    "y_train = diabetes_train['diabetes']\n",
    "X_test = diabetes_train.drop(columns=['diabetes', 'gender'])\n",
    "y_test = diabetes_train['diabetes']\n",
    "\n",
    "numerical_features = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']\n",
    "binary_features = ['hypertension', 'heart_disease'] ## No need to perform any preprocessing\n",
    "categorical_features = ['smoking_history', ]\n",
    "\n",
    "preprocessor_all = make_column_transformer(\n",
    "    (StandardScaler(), numerical_features),\n",
    "    (OrdinalEncoder(), categorical_features),\n",
    "    ('passthrough', binary_features)\n",
    ")\n",
    "\n",
    "X_train_transformed = preprocessor_all.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor_all.fit_transform(X_test)\n",
    "y_train_transformed = y_train\n",
    "y_test_transformed = y_test\n",
    "\n",
    "pipe = make_pipeline(preprocessor_all, KNeighborsClassifier())\n",
    "\n",
    "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "cv_scores_std = np.std(cv_scores)\n",
    "print(\"average cv score:\", cv_scores.mean(), \"±\", cv_scores_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fa7e76-3baf-4173-8d6e-a3290becb9c7",
   "metadata": {},
   "source": [
    "Our score has decreased from our previous model. Let's see if we can select an ideal combinations of features by manually sifting through all combinations and finding the set with the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5787eba-0900-4ed4-acc9-79805f1a81c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from itertools import combinations\n",
    "\n",
    "## Used ChatGPT to help iterate over all possible combinations of features as to select the top ten feature sets with the greatest score\n",
    "\n",
    "def score_feature_combinations(X, y, model, preprocessors):\n",
    "    scores = []\n",
    "\n",
    "    for r in range(1, len(X.columns) + 1):\n",
    "        for feature_combination in combinations(X.columns, r):\n",
    "            preprocessor = make_column_transformer(\n",
    "                (StandardScaler(), [feature for feature in feature_combination if feature in numerical_features]),\n",
    "                (OrdinalEncoder(), [feature for feature in feature_combination if feature in categorical_features]),\n",
    "                ('passthrough', [feature for feature in feature_combination if feature in binary_features])\n",
    "            )\n",
    "\n",
    "            pipe = make_pipeline(preprocessor, model)\n",
    "\n",
    "            avg_score = cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()\n",
    "            scores.append((feature_combination, avg_score))\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return scores\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "feature_combination_scores = score_feature_combinations(X_train, y_train, knn_model, preprocessor_all)\n",
    "\n",
    "for feature_combination, score in feature_combination_scores[:10]:\n",
    "    print(f\"Feature Combination: {feature_combination}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5336741-128c-4e63-811f-d374927c0ee2",
   "metadata": {
    "tags": []
   },
   "source": [
    "The feature combination of `'age', 'hypertension', 'heart_disease', 'bmi', 'HbA1c_level', 'blood_glucose_level'` produces a model with a score of 0.89098, slightly outperforming our model which considers all features (0.88988) and the model which only considers only numerical features with hyperparameter optimization (0.88306).\n",
    "\n",
    "There is other information we can infer from our feature combinations. `age, blood_glucose_level` and `HbA1c_level` seem to be the most important features to consider when predicting if a patient has diabetes as these three features all appear in top ten feature combinations. However, the addition of `hypertension`, `bmi` and/or `smoking_history` are necessary to help improve the accuracy of our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689d4113-9dcc-4763-8047-7d782ae7cfe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_opt = diabetes_train.drop(columns=['diabetes', 'gender', 'smoking_history'])\n",
    "y_train_opt = diabetes_train['diabetes']\n",
    "X_test_opt = diabetes_train.drop(columns=['diabetes', 'gender', 'smoking_history'])\n",
    "y_test_opt = diabetes_train['diabetes']\n",
    "\n",
    "numerical_features_opt = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']\n",
    "binary_features_opt = ['hypertension', 'heart_disease'] ## No need to perform any preprocessing\n",
    "\n",
    "preprocessor_opt = make_column_transformer(\n",
    "    (StandardScaler(), numerical_features_opt),\n",
    "    ('passthrough', binary_features_opt)\n",
    ")\n",
    "\n",
    "X_train_transformed_opt = preprocessor_opt.fit_transform(X_train_opt)\n",
    "X_test_transformed_opt = preprocessor_opt.fit_transform(X_test_opt)\n",
    "y_train_transformed_opt = y_train_opt\n",
    "y_test_transformed_opt = y_test_opt\n",
    "\n",
    "pipe_opt = make_pipeline(preprocessor_opt, KNeighborsClassifier())\n",
    "\n",
    "cv_scores_opt = cross_val_score(pipe_opt, X_train_opt, y_train_opt, cv=5, scoring='accuracy')\n",
    "\n",
    "cv_scores_opt_std = np.std(cv_scores)\n",
    "print(\"average cv score:\", cv_scores_opt.mean(), \"±\", cv_scores_opt_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd26669-e843-485c-bfa3-012cdeabe715",
   "metadata": {},
   "source": [
    "To further improve our model accuracy, we could perform hyperparamater optimization on `n_neighbors`. However due to time constraints, we will continue working with our model using the default value of `n_neighbors=5`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e27fc-c88c-43c7-87bd-b8c2127f6bd8",
   "metadata": {},
   "source": [
    "TODO:  compare test score with numerical features only model, use model on new observation(s), write a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d126d23-ebf5-4071-8e4e-8557eac19797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: test scores for numerical features only, knn=31, and for optimized feature set\n",
    "diabetes_pipe_knn_31.fit(X_train, y_train)\n",
    "\n",
    "X_test = diabetes_test[[\"age\", \"bmi\", \"HbA1c_level\", \"blood_glucose_level\"]]\n",
    "y_test = diabetes_test[\"diabetes\"]\n",
    "\n",
    "test_score = diabetes_pipe_knn_31.score(X_test, y_test)\n",
    "print(\"Test set accuracy:\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92672de5-b4d6-4eae-840c-f327971799a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: new observation(s) for both numerical features only, and for optimized feature set, discuss findings and results\n",
    "\n",
    "# new_observation = pd.DataFrame({\n",
    "#     \"age\": [26.0],  \n",
    "#     \"bmi\": [25.5],\n",
    "#     \"HbA1c_level\": [1.8],\n",
    "#     \"blood_glucose_level\": [170.0]\n",
    "# })\n",
    "\n",
    "# knn_fit = make_pipeline(diabetes_preprocessor, KNeighborsClassifier(n_neighbors=33)).fit(\n",
    "#     diabetes_downsampled[feature_names],\n",
    "#     diabetes_downsampled[\"diabetes\"]\n",
    "# )\n",
    "\n",
    "# new_prediction = knn_fit.predict(new_observation)\n",
    "# new_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea6b60a-0541-4f8d-a87a-9718ace1f8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590247c2-57fc-44ff-b6c0-3f4d8dd6cbc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
